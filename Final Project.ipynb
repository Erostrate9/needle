{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Final Project - Transformer Implementation\n",
    "**Authors**:\n",
    "* Yuxuan Sun: <yuxuan_eric_sun@outlook.com>\n",
    "* Sergey: <seriy.karp2@gmail.com>\n",
    "* Haitao Gao: <haitaogao423@gmail.com>\n",
    "\n",
    "**Code Repository**: <https://github.com/Erostrate9/needle>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "To sum up what we learned during *10-714: Deep Learning Systems*, we've implemented the Transformer architecture and its corresponding modules with our self-made *needle*.\n",
    "\n",
    "The overall goal of our *Final Project* is to implement the trainable Transformer architecture [1], which can be divided into some ingredients — Multi-Head Attention, Self-Attention and Positional Encoding, and The Transformer Architecture (Positionwise Feed-Forward Networks, Residual Connection and Layer Normalization, Transformer Encoder Block & Encoder, Transformer Decoder Block & Decoder, and Encoder-Decoder Seq2Seq model.)\n",
    "\n",
    "In this project, in order to simplify the verification of numerical correctness, we use the implementation of d2l.ai [2] for reference, and its PyTorch implementation [3] has been cited for comparison.\n",
    "\n",
    "[1]: Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. \"Attention is all you need.\" Advances in neural information processing systems 30, 2017.\n",
    "[2]: Zhang, Aston, Zachary C. Lipton, Mu Li, and Alexander J. Smola. \"Dive into deep learning.\" arXiv preprint arXiv:2106.11342, 2021.\n",
    "[3]: Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J., “Releases v0.17.5 · D2L-ai/D2L-en-pytorch,” GitHub, 27-May-2022. [Online]. Available: <https://github.com/d2l-ai/d2l-en/releases/download/v0.17.5/d2l-en-pytorch.pdf>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Needle\n",
    "We've implemented a fine needle with cpu/cuda ndarray backend."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m============================= test session starts ==============================\u001B[0m\r\n",
      "platform linux -- Python 3.9.15, pytest-7.1.2, pluggy-1.0.0 -- /home/erostrate9/anaconda3/envs/dls/bin/python3\r\n",
      "cachedir: .pytest_cache\r\n",
      "rootdir: /home/erostrate9/PycharmProjects/needle\r\n",
      "plugins: anyio-3.5.0\r\n",
      "collected 1853 items / 1685 deselected / 168 selected                          \u001B[0m\u001B[1m\r\n",
      "\r\n",
      "tests/test_nd_backend.py::test_ewise_fn[cpu-shape0-divide] \u001B[32mPASSED\u001B[0m\u001B[32m        [  0%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_ewise_fn[cpu-shape0-subtract] \u001B[32mPASSED\u001B[0m\u001B[32m      [  1%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_ewise_fn[cpu-shape1-divide] \u001B[32mPASSED\u001B[0m\u001B[32m        [  1%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_ewise_fn[cpu-shape1-subtract] \u001B[32mPASSED\u001B[0m\u001B[32m      [  2%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_ewise_fn[cuda-shape0-divide] \u001B[32mPASSED\u001B[0m\u001B[32m       [  2%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_ewise_fn[cuda-shape0-subtract] \u001B[32mPASSED\u001B[0m\u001B[32m     [  3%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_ewise_fn[cuda-shape1-divide] \u001B[32mPASSED\u001B[0m\u001B[32m       [  4%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_ewise_fn[cuda-shape1-subtract] \u001B[32mPASSED\u001B[0m\u001B[32m     [  4%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_scalar_fn[cpu-shape0-divide] \u001B[32mPASSED\u001B[0m\u001B[32m       [  5%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_scalar_fn[cpu-shape0-subtract] \u001B[32mPASSED\u001B[0m\u001B[32m     [  5%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_scalar_fn[cpu-shape1-divide] \u001B[32mPASSED\u001B[0m\u001B[32m       [  6%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_scalar_fn[cpu-shape1-subtract] \u001B[32mPASSED\u001B[0m\u001B[32m     [  7%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_scalar_fn[cuda-shape0-divide] \u001B[32mPASSED\u001B[0m\u001B[32m      [  7%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_scalar_fn[cuda-shape0-subtract] \u001B[32mPASSED\u001B[0m\u001B[32m    [  8%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_scalar_fn[cuda-shape1-divide] \u001B[32mPASSED\u001B[0m\u001B[32m      [  8%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_scalar_fn[cuda-shape1-subtract] \u001B[32mPASSED\u001B[0m\u001B[32m    [  9%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_matmul[cpu-16-16-16] \u001B[32mPASSED\u001B[0m\u001B[32m               [ 10%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_matmul[cpu-8-8-8] \u001B[32mPASSED\u001B[0m\u001B[32m                  [ 10%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_matmul[cpu-1-2-3] \u001B[32mPASSED\u001B[0m\u001B[32m                  [ 11%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_matmul[cpu-3-4-5] \u001B[32mPASSED\u001B[0m\u001B[32m                  [ 11%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_matmul[cpu-5-4-3] \u001B[32mPASSED\u001B[0m\u001B[32m                  [ 12%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_matmul[cpu-16-16-32] \u001B[32mPASSED\u001B[0m\u001B[32m               [ 13%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_matmul[cpu-64-64-64] \u001B[32mPASSED\u001B[0m\u001B[32m               [ 13%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_matmul[cpu-72-72-72] \u001B[32mPASSED\u001B[0m\u001B[32m               [ 14%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_matmul[cpu-72-73-74] \u001B[32mPASSED\u001B[0m\u001B[32m               [ 14%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_matmul[cpu-74-73-72] \u001B[32mPASSED\u001B[0m\u001B[32m               [ 15%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_matmul[cpu-128-128-128] \u001B[32mPASSED\u001B[0m\u001B[32m            [ 16%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_matmul[cuda-16-16-16] \u001B[32mPASSED\u001B[0m\u001B[32m              [ 16%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_matmul[cuda-8-8-8] \u001B[32mPASSED\u001B[0m\u001B[32m                 [ 17%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_matmul[cuda-1-2-3] \u001B[32mPASSED\u001B[0m\u001B[32m                 [ 17%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_matmul[cuda-3-4-5] \u001B[32mPASSED\u001B[0m\u001B[32m                 [ 18%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_matmul[cuda-5-4-3] \u001B[32mPASSED\u001B[0m\u001B[32m                 [ 19%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_matmul[cuda-16-16-32] \u001B[32mPASSED\u001B[0m\u001B[32m              [ 19%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_matmul[cuda-64-64-64] \u001B[32mPASSED\u001B[0m\u001B[32m              [ 20%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_matmul[cuda-72-72-72] \u001B[32mPASSED\u001B[0m\u001B[32m              [ 20%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_matmul[cuda-72-73-74] \u001B[32mPASSED\u001B[0m\u001B[32m              [ 21%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_matmul[cuda-74-73-72] \u001B[32mPASSED\u001B[0m\u001B[32m              [ 22%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_matmul[cuda-128-128-128] \u001B[32mPASSED\u001B[0m\u001B[32m           [ 22%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_batch_matmul[cpu-3-3-4-16-16] \u001B[32mPASSED\u001B[0m\u001B[32m      [ 23%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_batch_matmul[cpu-5-5-4-8-8] \u001B[32mPASSED\u001B[0m\u001B[32m        [ 23%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_batch_matmul[cpu-3-1-2-2-3] \u001B[32mPASSED\u001B[0m\u001B[32m        [ 24%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_batch_matmul[cpu-4-3-3-4-5] \u001B[32mPASSED\u001B[0m\u001B[32m        [ 25%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_batch_matmul[cpu-1-5-5-4-3] \u001B[32mPASSED\u001B[0m\u001B[32m        [ 25%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_batch_matmul[cpu-11-16-15-16-32] \u001B[32mPASSED\u001B[0m\u001B[32m   [ 26%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_batch_matmul[cpu-12-64-64-64-64] \u001B[32mPASSED\u001B[0m\u001B[32m   [ 26%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_batch_matmul[cuda-3-3-4-16-16] \u001B[32mPASSED\u001B[0m\u001B[32m     [ 27%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_batch_matmul[cuda-5-5-4-8-8] \u001B[32mPASSED\u001B[0m\u001B[32m       [ 27%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_batch_matmul[cuda-3-1-2-2-3] \u001B[32mPASSED\u001B[0m\u001B[32m       [ 28%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_batch_matmul[cuda-4-3-3-4-5] \u001B[32mPASSED\u001B[0m\u001B[32m       [ 29%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_batch_matmul[cuda-1-5-5-4-3] \u001B[32mPASSED\u001B[0m\u001B[32m       [ 29%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_batch_matmul[cuda-11-16-15-16-32] \u001B[32mPASSED\u001B[0m\u001B[32m  [ 30%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_batch_matmul[cuda-12-64-64-64-64] \u001B[32mPASSED\u001B[0m\u001B[32m  [ 30%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_power[cpu-shape0] \u001B[32mPASSED\u001B[0m\u001B[32m                  [ 31%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_power[cpu-shape1] \u001B[32mPASSED\u001B[0m\u001B[32m                  [ 32%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_power[cuda-shape0] \u001B[32mPASSED\u001B[0m\u001B[32m                 [ 32%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_power[cuda-shape1] \u001B[32mPASSED\u001B[0m\u001B[32m                 [ 33%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_log[cpu-shape0] \u001B[32mPASSED\u001B[0m\u001B[32m                    [ 33%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_log[cpu-shape1] \u001B[32mPASSED\u001B[0m\u001B[32m                    [ 34%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_log[cuda-shape0] \u001B[32mPASSED\u001B[0m\u001B[32m                   [ 35%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_log[cuda-shape1] \u001B[32mPASSED\u001B[0m\u001B[32m                   [ 35%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_exp[cpu-shape0] \u001B[32mPASSED\u001B[0m\u001B[32m                    [ 36%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_exp[cpu-shape1] \u001B[32mPASSED\u001B[0m\u001B[32m                    [ 36%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_exp[cuda-shape0] \u001B[32mPASSED\u001B[0m\u001B[32m                   [ 37%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_exp[cuda-shape1] \u001B[32mPASSED\u001B[0m\u001B[32m                   [ 38%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_exp_backward[cpu-shape0] \u001B[32mPASSED\u001B[0m\u001B[32m           [ 38%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_exp_backward[cpu-shape1] \u001B[32mPASSED\u001B[0m\u001B[32m           [ 39%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_exp_backward[cuda-shape0] \u001B[32mPASSED\u001B[0m\u001B[32m          [ 39%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_exp_backward[cuda-shape1] \u001B[32mPASSED\u001B[0m\u001B[32m          [ 40%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_relu[cpu-shape0] \u001B[32mPASSED\u001B[0m\u001B[32m                   [ 41%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_relu[cpu-shape1] \u001B[32mPASSED\u001B[0m\u001B[32m                   [ 41%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_relu[cuda-shape0] \u001B[32mPASSED\u001B[0m\u001B[32m                  [ 42%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_relu[cuda-shape1] \u001B[32mPASSED\u001B[0m\u001B[32m                  [ 42%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_relu_backward[cpu-shape0] \u001B[32mPASSED\u001B[0m\u001B[32m          [ 43%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_relu_backward[cpu-shape1] \u001B[32mPASSED\u001B[0m\u001B[32m          [ 44%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_relu_backward[cuda-shape0] \u001B[32mPASSED\u001B[0m\u001B[32m         [ 44%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_relu_backward[cuda-shape1] \u001B[32mPASSED\u001B[0m\u001B[32m         [ 45%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_tanh[cpu-shape0] \u001B[32mPASSED\u001B[0m\u001B[32m                   [ 45%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_tanh[cpu-shape1] \u001B[32mPASSED\u001B[0m\u001B[32m                   [ 46%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_tanh[cuda-shape0] \u001B[32mPASSED\u001B[0m\u001B[32m                  [ 47%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_tanh[cuda-shape1] \u001B[32mPASSED\u001B[0m\u001B[32m                  [ 47%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_tanh_backward[cpu-shape0] \u001B[32mPASSED\u001B[0m\u001B[32m          [ 48%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_tanh_backward[cpu-shape1] \u001B[32mPASSED\u001B[0m\u001B[32m          [ 48%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_tanh_backward[cuda-shape0] \u001B[32mPASSED\u001B[0m\u001B[32m         [ 49%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_tanh_backward[cuda-shape1] \u001B[32mPASSED\u001B[0m\u001B[32m         [ 50%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_stack[cpu-shape0-0-1] \u001B[32mPASSED\u001B[0m\u001B[32m              [ 50%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_stack[cpu-shape1-0-2] \u001B[32mPASSED\u001B[0m\u001B[32m              [ 51%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_stack[cpu-shape2-2-5] \u001B[32mPASSED\u001B[0m\u001B[32m              [ 51%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_stack[cuda-shape0-0-1] \u001B[32mPASSED\u001B[0m\u001B[32m             [ 52%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_stack[cuda-shape1-0-2] \u001B[32mPASSED\u001B[0m\u001B[32m             [ 52%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_stack[cuda-shape2-2-5] \u001B[32mPASSED\u001B[0m\u001B[32m             [ 53%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_stack_backward[cpu-shape0-0-1] \u001B[32mPASSED\u001B[0m\u001B[32m     [ 54%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_stack_backward[cpu-shape1-0-2] \u001B[32mPASSED\u001B[0m\u001B[32m     [ 54%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_stack_backward[cpu-shape2-2-5] \u001B[32mPASSED\u001B[0m\u001B[32m     [ 55%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_stack_backward[cuda-shape0-0-1] \u001B[32mPASSED\u001B[0m\u001B[32m    [ 55%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_stack_backward[cuda-shape1-0-2] \u001B[32mPASSED\u001B[0m\u001B[32m    [ 56%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_stack_backward[cuda-shape2-2-5] \u001B[32mPASSED\u001B[0m\u001B[32m    [ 57%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_summation[cpu-shape0-None] \u001B[32mPASSED\u001B[0m\u001B[32m         [ 57%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_summation[cpu-shape1-0] \u001B[32mPASSED\u001B[0m\u001B[32m            [ 58%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_summation[cpu-shape2-1] \u001B[32mPASSED\u001B[0m\u001B[32m            [ 58%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_summation[cpu-shape3-2] \u001B[32mPASSED\u001B[0m\u001B[32m            [ 59%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_summation[cuda-shape0-None] \u001B[32mPASSED\u001B[0m\u001B[32m        [ 60%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_summation[cuda-shape1-0] \u001B[32mPASSED\u001B[0m\u001B[32m           [ 60%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_summation[cuda-shape2-1] \u001B[32mPASSED\u001B[0m\u001B[32m           [ 61%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_summation[cuda-shape3-2] \u001B[32mPASSED\u001B[0m\u001B[32m           [ 61%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_summation_backward[cpu-shape0-None] \u001B[32mPASSED\u001B[0m\u001B[32m [ 62%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_summation_backward[cpu-shape1-0] \u001B[32mPASSED\u001B[0m\u001B[32m   [ 63%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_summation_backward[cpu-shape2-1] \u001B[32mPASSED\u001B[0m\u001B[32m   [ 63%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_summation_backward[cpu-shape3-2] \u001B[32mPASSED\u001B[0m\u001B[32m   [ 64%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_summation_backward[cuda-shape0-None] \u001B[32mPASSED\u001B[0m\u001B[32m [ 64%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_summation_backward[cuda-shape1-0] \u001B[32mPASSED\u001B[0m\u001B[32m  [ 65%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_summation_backward[cuda-shape2-1] \u001B[32mPASSED\u001B[0m\u001B[32m  [ 66%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_summation_backward[cuda-shape3-2] \u001B[32mPASSED\u001B[0m\u001B[32m  [ 66%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_max[cpu-shape0-None] \u001B[32mPASSED\u001B[0m\u001B[32m               [ 67%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_max[cpu-shape1-0] \u001B[32mPASSED\u001B[0m\u001B[32m                  [ 67%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_max[cpu-shape2-1] \u001B[32mPASSED\u001B[0m\u001B[32m                  [ 68%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_max[cpu-shape3-2] \u001B[32mPASSED\u001B[0m\u001B[32m                  [ 69%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_max[cuda-shape0-None] \u001B[32mPASSED\u001B[0m\u001B[32m              [ 69%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_max[cuda-shape1-0] \u001B[32mPASSED\u001B[0m\u001B[32m                 [ 70%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_max[cuda-shape2-1] \u001B[32mPASSED\u001B[0m\u001B[32m                 [ 70%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_max[cuda-shape3-2] \u001B[32mPASSED\u001B[0m\u001B[32m                 [ 71%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_max_backward[cpu-shape0-None] \u001B[32mPASSED\u001B[0m\u001B[32m      [ 72%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_max_backward[cpu-shape1-0] \u001B[32mPASSED\u001B[0m\u001B[32m         [ 72%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_max_backward[cpu-shape2-1] \u001B[32mPASSED\u001B[0m\u001B[32m         [ 73%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_max_backward[cpu-shape3-2] \u001B[32mPASSED\u001B[0m\u001B[32m         [ 73%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_max_backward[cuda-shape0-None] \u001B[32mPASSED\u001B[0m\u001B[32m     [ 74%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_max_backward[cuda-shape1-0] \u001B[32mPASSED\u001B[0m\u001B[32m        [ 75%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_max_backward[cuda-shape2-1] \u001B[32mPASSED\u001B[0m\u001B[32m        [ 75%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_max_backward[cuda-shape3-2] \u001B[32mPASSED\u001B[0m\u001B[32m        [ 76%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_broadcast_to[cpu-shape0-shape_to0] \u001B[32mPASSED\u001B[0m\u001B[32m [ 76%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_broadcast_to[cpu-shape1-shape_to1] \u001B[32mPASSED\u001B[0m\u001B[32m [ 77%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_broadcast_to[cuda-shape0-shape_to0] \u001B[32mPASSED\u001B[0m\u001B[32m [ 77%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_broadcast_to[cuda-shape1-shape_to1] \u001B[32mPASSED\u001B[0m\u001B[32m [ 78%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_reshape[cpu-shape0-shape_to0] \u001B[32mPASSED\u001B[0m\u001B[32m      [ 79%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_reshape[cpu-shape1-shape_to1] \u001B[32mPASSED\u001B[0m\u001B[32m      [ 79%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_reshape[cuda-shape0-shape_to0] \u001B[32mPASSED\u001B[0m\u001B[32m     [ 80%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_reshape[cuda-shape1-shape_to1] \u001B[32mPASSED\u001B[0m\u001B[32m     [ 80%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_transpose[cpu-axes0-shape0] \u001B[32mPASSED\u001B[0m\u001B[32m        [ 81%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_transpose[cpu-axes0-shape1] \u001B[32mPASSED\u001B[0m\u001B[32m        [ 82%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_transpose[cpu-axes1-shape0] \u001B[32mPASSED\u001B[0m\u001B[32m        [ 82%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_transpose[cpu-axes1-shape1] \u001B[32mPASSED\u001B[0m\u001B[32m        [ 83%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_transpose[cpu-None-shape0] \u001B[32mPASSED\u001B[0m\u001B[32m         [ 83%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_transpose[cpu-None-shape1] \u001B[32mPASSED\u001B[0m\u001B[32m         [ 84%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_transpose[cuda-axes0-shape0] \u001B[32mPASSED\u001B[0m\u001B[32m       [ 85%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_transpose[cuda-axes0-shape1] \u001B[32mPASSED\u001B[0m\u001B[32m       [ 85%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_transpose[cuda-axes1-shape0] \u001B[32mPASSED\u001B[0m\u001B[32m       [ 86%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_transpose[cuda-axes1-shape1] \u001B[32mPASSED\u001B[0m\u001B[32m       [ 86%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_transpose[cuda-None-shape0] \u001B[32mPASSED\u001B[0m\u001B[32m        [ 87%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_transpose[cuda-None-shape1] \u001B[32mPASSED\u001B[0m\u001B[32m        [ 88%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_logsumexp[cpu-shape0-None] \u001B[32mPASSED\u001B[0m\u001B[32m         [ 88%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_logsumexp[cpu-shape1-0] \u001B[32mPASSED\u001B[0m\u001B[32m            [ 89%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_logsumexp[cpu-shape2-1] \u001B[32mPASSED\u001B[0m\u001B[32m            [ 89%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_logsumexp[cpu-shape3-2] \u001B[32mPASSED\u001B[0m\u001B[32m            [ 90%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_logsumexp[cuda-shape0-None] \u001B[32mPASSED\u001B[0m\u001B[32m        [ 91%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_logsumexp[cuda-shape1-0] \u001B[32mPASSED\u001B[0m\u001B[32m           [ 91%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_logsumexp[cuda-shape2-1] \u001B[32mPASSED\u001B[0m\u001B[32m           [ 92%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_logsumexp[cuda-shape3-2] \u001B[32mPASSED\u001B[0m\u001B[32m           [ 92%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_logsumexp_backward[cpu-shape0-None] \u001B[32mPASSED\u001B[0m\u001B[32m [ 93%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_logsumexp_backward[cpu-shape1-0] \u001B[32mPASSED\u001B[0m\u001B[32m   [ 94%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_logsumexp_backward[cpu-shape2-1] \u001B[32mPASSED\u001B[0m\u001B[32m   [ 94%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_logsumexp_backward[cpu-shape3-2] \u001B[32mPASSED\u001B[0m\u001B[32m   [ 95%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_logsumexp_backward[cuda-shape0-None] \u001B[32mPASSED\u001B[0m\u001B[32m [ 95%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_logsumexp_backward[cuda-shape1-0] \u001B[32mPASSED\u001B[0m\u001B[32m  [ 96%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_logsumexp_backward[cuda-shape2-1] \u001B[32mPASSED\u001B[0m\u001B[32m  [ 97%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_logsumexp_backward[cuda-shape3-2] \u001B[32mPASSED\u001B[0m\u001B[32m  [ 97%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_logsumexp_submit[cpu-shape0-0] \u001B[32mPASSED\u001B[0m\u001B[32m     [ 98%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_logsumexp_submit[cpu-shape1-3] \u001B[32mPASSED\u001B[0m\u001B[32m     [ 98%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_logsumexp_submit[cuda-shape0-0] \u001B[32mPASSED\u001B[0m\u001B[32m    [ 99%]\u001B[0m\r\n",
      "tests/test_nd_backend.py::test_logsumexp_submit[cuda-shape1-3] \u001B[32mPASSED\u001B[0m\u001B[32m    [100%]\u001B[0m\r\n",
      "\r\n",
      "\u001B[32m===================== \u001B[32m\u001B[1m168 passed\u001B[0m, \u001B[33m1685 deselected\u001B[0m\u001B[32m in 2.35s\u001B[0m\u001B[32m =====================\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./python')\n",
    "sys.path.append('./apps')\n",
    "import needle as ndl\n",
    "import numpy as np\n",
    "\n",
    "!python3 -m pytest -l -v -k \"nd_backend\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multi-Head Attention\n",
    "![Multi-head attention, where multiple heads are concatenated then linearly transformed.[2, Fig. 11.5.1]](https://d2l.ai/_images/multi-head-attention.svg)\n",
    "### Model\n",
    "As Prof. Zico Kolter has introduced the mechanism of self-attention and Transformer in Lecture 20 & 21, we'll briefly describe the theory part in this report and focus on its implementation.\n",
    "According to [4], the practical implementation of attention normally used is what we called *multi-head attention*, which means that we run the self-attention mechanism of different subsets of the $K$, $Q$, $V$ terms, then concatenate them together.  Formally, we'll partition these terms as\n",
    "\\begin{equation}\n",
    "K = \\begin{bmatrix} K_1 & K_2 & \\cdots & K_{\\mathrm{heads}} \\end{bmatrix}\n",
    "\\end{equation}\n",
    "(and similarly for $Q$ and $V$.\n",
    "\n",
    "Then will form the self attention outputs\n",
    "\\begin{equation}\n",
    "Y_i = \\mathrm{softmax}\\left(\\frac{K_iQ_i^T}{\\sqrt{d/\\mathrm{heads}}}\\right)V_i\n",
    "\\end{equation}\n",
    "and then form the final ouput\n",
    "\\begin{equation}\n",
    "Y = \\begin{bmatrix} Y_1 & Y_2 & \\cdots & Y_{\\mathrm{heads}} \\end{bmatrix} W_o.\n",
    "\\end{equation}\n",
    "\n",
    "[4]: Z. Kolter, “Public_notebooks/transformer_implementation.ipynb at main · dlsyscourse/public_notebooks,” Deep Learning Systems 21 - Transformers + Attention Implementation, 15-Nov-2022. [Online]. Available: https://github.com/dlsyscourse/public_notebooks/blob/main/transformer_implementation.ipynb.\n",
    "\n",
    "### Implementation\n",
    "First, we\n",
    "For each head of the multi-head attention, we choose the scaled dot-product attention, where a masked softmax operation is used to output a probability distribution as attention weights.\n",
    "```python\n",
    "def masked_softmax(X, valid_lens):\n",
    "    # valid_lens: numpy array\n",
    "    def _sequence_mask(X: Tensor, valid_lens, value=0):\n",
    "        # X: n * d\n",
    "        maxlen = X.shape[-1]\n",
    "        mask = (np.arange(maxlen)[None, :] < valid_lens[:, None])\n",
    "        mask_mul = mask.astype(np.float32)\n",
    "        mask_add = (~mask).astype(np.float32) * value\n",
    "        mask_mul = Tensor(mask_mul, device=X.device, dtype=X.dtype, requires_grad=False)\n",
    "        mask_add = Tensor(mask_add, device=X.device, dtype=X.dtype, requires_grad=False)\n",
    "        return X * mask_mul + mask_add\n",
    "\n",
    "    if valid_lens is None:\n",
    "        return ops.softmax(X)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if len(valid_lens.shape) == 1:\n",
    "            assert valid_lens.shape[0] == X.shape[0]\n",
    "            valid_lens = valid_lens.repeat(X.shape[1])\n",
    "        else:\n",
    "            valid_lens = valid_lens.reshape(-1)\n",
    "        # On the last axis, replace masked elements with a very large negative\n",
    "        # value, whose exponentiation outputs 0\n",
    "        X = _sequence_mask(X.reshape((prod(shape[:-1]), shape[-1])), valid_lens, value=-1e6)\n",
    "        return ops.softmax(X.reshape(shape))\n",
    "\n",
    "class DotProductAttention(Module):\n",
    "    \"\"\"Scaled dot product attention.\"\"\"\n",
    "\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = Dropout(dropout)\n",
    "\n",
    "    # Shape of queries: (batch_size, no. of queries, d)\n",
    "    # Shape of keys: (batch_size, no. of key-value pairs, d)\n",
    "    # Shape of values: (batch_size, no. of key-value pairs, value dimension)\n",
    "    # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        d = queries.shape[-1]\n",
    "        # Swap the last two dimensions of keys with keys.transpose(1, 2)\n",
    "        scores = ops.bmm(queries, keys.transpose((1, 2))) / math.sqrt(d)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        return ops.bmm(self.dropout(self.attention_weights), values)\n",
    "```\n",
    "```python\n",
    "class MultiHeadAttention(Module):\n",
    "    \"\"\"Multi-head attention.\"\"\"\n",
    "\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens,\n",
    "                 num_heads, dropout, bias=False, device=None, dtype=\"float32\"):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = DotProductAttention(dropout)\n",
    "        self.W_q = Linear(query_size, num_hiddens, bias=bias, device=device, dtype=dtype)\n",
    "        self.W_k = Linear(key_size, num_hiddens, bias=bias, device=device, dtype=dtype)\n",
    "        self.W_v = Linear(value_size, num_hiddens, bias=bias, device=device, dtype=dtype)\n",
    "        self.W_o = Linear(num_hiddens, num_hiddens, bias=bias, device=device, dtype=dtype)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        # Shape of queries, keys, or values:\n",
    "        # (batch_size, no. of queries or key-value pairs, num_hiddens)\n",
    "        # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n",
    "        # After transposing, shape of output queries, keys, or values:\n",
    "        # (batch_size * num_heads, no. of queries or key-value pairs,\n",
    "        # num_hiddens / num_heads)\n",
    "        queries = self.transpose_qkv(self.W_q(queries))\n",
    "        keys = self.transpose_qkv(self.W_k(keys))\n",
    "        values = self.transpose_qkv(self.W_v(values))\n",
    "        if valid_lens is not None:\n",
    "            # On axis 0, copy the first item (scalar or vector) for num_heads\n",
    "            # times, then copy the next item, and so on\n",
    "            valid_lens = valid_lens.repeat(repeats=self.num_heads, axis=0)\n",
    "\n",
    "        # Shape of output: (batch_size * num_heads, no. of queries,\n",
    "        # num_hiddens / num_heads)\n",
    "        output = self.attention(queries, keys, values, valid_lens)\n",
    "        # Shape of output_concat: (batch_size, no. of queries, num_hiddens)\n",
    "        output_concat = self.transpose_output(output)\n",
    "        return self.W_o(output_concat)\n",
    "\n",
    "    def transpose_qkv(self, X):\n",
    "        \"\"\"Transposition for parallel computation of multiple attention heads.\"\"\"\n",
    "        # Shape of input X: (batch_size, no. of queries or key-value pairs,\n",
    "        # num_hiddens). Shape of output X: (batch_size, no. of queries or\n",
    "        # key-value pairs, num_heads, num_hiddens / num_heads)\n",
    "        X = X.reshape((X.shape[0], X.shape[1], self.num_heads, -1))\n",
    "        # Shape of output X: (batch_size, num_heads, no. of queries or key-value\n",
    "        # pairs, num_hiddens / num_heads)\n",
    "        X = X.permute((0, 2, 1, 3))\n",
    "        # Shape of output: (batch_size * num_heads, no. of queries or key-value\n",
    "        # pairs, num_hiddens / num_heads)\n",
    "        X = X.reshape((-1, X.shape[2], X.shape[3]))\n",
    "        return X\n",
    "\n",
    "    def transpose_output(self, X):\n",
    "        \"\"\"Reverse the operation of transpose_qkv.\"\"\"\n",
    "        X = X.reshape((-1, self.num_heads, X.shape[1], X.shape[2]))\n",
    "        X = X.permute((0, 2, 1, 3))\n",
    "        return X.reshape((X.shape[0], X.shape[1], -1))\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
